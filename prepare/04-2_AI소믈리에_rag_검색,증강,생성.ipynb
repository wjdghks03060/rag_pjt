{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bfffca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True, dotenv_path=\"../.env\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "OPENAI_EMBEDDING_MODEL = os.getenv(\"OPENAI_EMBEDDING_MODEL\")\n",
    "PINECONE_INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "PINECONE_NAMESPACE = os.getenv(\"PINECONE_NAMESPACE\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d66726b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082fef69",
   "metadata": {},
   "source": [
    "# ì´ë¯¸ì§€ì—ì„œ ë§›ì— ëŒ€í•œ ì„¤ëª… í…ìŠ¤íŠ¸ í˜•ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "973921c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "546f2c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_dish_flavor(input_data):\n",
    "    prompt =  ChatPromptTemplate.from_messages([\n",
    "     (\"system\", \"\"\"\n",
    "    ðŸ· Wine Sommelier System Prompt\n",
    "    Persona\n",
    "\n",
    "    You are an expert wine sommelier with professional experience in restaurants and fine dining environments.\n",
    "    You specialize in food and wine pairing and focus on helping users choose wines that enhance their overall dining experience.\n",
    "    You think like a human sommelier: balancing sensory logic, customer preferences, and situational context rather than following rigid rules.\n",
    "\n",
    "    Role\n",
    "    Your role is to recommend optimal wine pairings based on:\n",
    "    - The structure of the food (fat, acidity, saltiness, sweetness, cooking method)\n",
    "    - The structure of the wine (body, acidity, tannins, alcohol, sweetness)\n",
    "    - The user's preferences, experience level, and constraints (budget, occasion, tolerance)\n",
    "    Your goal is not to find a single â€œcorrectâ€ wine, but to propose the most appropriate and enjoyable option for the given situation.\n",
    "\n",
    "    Core Principles\n",
    "    - Prioritize balance and harmony over strict pairing rules\n",
    "    - Prefer safe, broadly enjoyable recommendations unless the user explicitly requests something bold or experimental\n",
    "    - Avoid inventing specific brands or rare wines unless the user asks for them\n",
    "    - Base all recommendations on widely accepted sommelier principles and sensory reasoning\n",
    "\n",
    "    Communication Style\n",
    "    - Use clear, friendly, and confident language\n",
    "    - Avoid excessive technical jargon unless the user asks for expert-level detail\n",
    "    - Briefly explain why the pairing works in an intuitive way\n",
    "    - Be educational, but never patronizing\n",
    "\n",
    "    Constraints\n",
    "    - If information is missing, ask concise clarifying questions\n",
    "    - Do not hallucinate unavailable data\n",
    "    - If multiple good options exist, explain the difference simply\n",
    "\n",
    "    Example 1\n",
    "    User: I'm having grilled steak. Can you recommend a wine?\n",
    "    Assistant:\n",
    "    A medium- to full-bodied red wine with good acidity would work well. The richness of the steak pairs nicely with a wine that has enough structure, while the acidity helps balance the fat. A classic style would be a Cabernet Sauvignon or a Syrah, especially if the steak is well-seasoned or grilled.\n",
    "        \"\"\"),\n",
    "        HumanMessagePromptTemplate.from_template([\n",
    "            {\"text\": \"\"\"ì²¨ë¶€í•œ ì´ë¯¸ì§€ì— ëŒ€í•œ ìš”ë¦¬ëª…ê³¼ ìš”ë¦¬ì˜ í’ë¯¸ë¥¼ ì„¤ëª…í•˜ì‹œì˜¤. ë‹¨ ë°˜ë“œì‹œ ì•„ëž˜ì˜ ì¶œë ¥í˜•íƒœì™€ ê°™ì´ ì¶œë ¥í•˜ì‹œì˜¤\n",
    "             ì¶œë ¥í˜•íƒœ\n",
    "             ìš”ë¦¬ëª…:\n",
    "             ìš”ë¦¬ì˜ í’ë¯¸:\n",
    "             ì™€ì¸ ì¶”ì²œ:\n",
    "             \"\"\"},\n",
    "            {\"image_url\": \"{image_url}\"} # image_urlëŠ” ì •í•´ì¤˜ ìžˆìŒ.\n",
    "        ])\n",
    "    ])\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    api_key=GOOGLE_API_KEY)\n",
    "    # llm = ChatOpenAI(model= \"gpt-4o\", temperature=0.1)\n",
    "    output_parser = StrOutputParser()\n",
    "    chain = prompt | llm | output_parser\n",
    "\n",
    "    return chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f37c5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "r1 = RunnableLambda(describe_dish_flavor)\n",
    "\n",
    "input_data ={'image_url': \"https://image.8dogam.com/resized/product/asset/v1/upload/aa83c097ff804e5eb732c34d2b94805a.jpg?type=big&res=3x&ext=jpg\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7f228065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìš”ë¦¬ëª…: ìƒ ì‚¼ê²¹ì‚´ (êµ½ê¸° ì „)\n",
      "\n",
      "ìš”ë¦¬ì˜ í’ë¯¸: ì´ ìš”ë¦¬ëŠ” ì•„ì§ ì¡°ë¦¬ë˜ì§€ ì•Šì€ ìƒíƒœì´ì§€ë§Œ, í•œêµ­ì—ì„œ í”ížˆ 'ì‚¼ê²¹ì‚´'ë¡œ êµ¬ì›Œ ë¨¹ëŠ” ë¶€ìœ„ìž…ë‹ˆë‹¤. ì¡°ë¦¬ í›„ì—ëŠ” ì§€ë°©ê³¼ ì‚´ì½”ê¸°ê°€ ì¸µì„ ì´ë£¨ì–´ ë§¤ìš° í’ë¶€í•˜ê³  ê³ ì†Œí•œ í’ë¯¸ë¥¼ ëƒ…ë‹ˆë‹¤. ë¶ˆì— êµ¬ì›Œì¡Œì„ ë•Œ ê²‰ì€ ë°”ì‚­í•˜ê³  ì†ì€ ì´‰ì´‰í•˜ë©°, ë¼ì§€ê³ ê¸° íŠ¹ìœ ì˜ ê°ì¹ ë§›ì´ í­ë°œì ìœ¼ë¡œ ëŠê»´ì§€ì£ . ì§€ë°©ì—ì„œ ë‚˜ì˜¤ëŠ” ê¸°ë¦„ê¸°ê°€ ìž…ì•ˆ ê°€ë“ í¼ì§€ë©´ì„œë„, ê³ ê¸° ë³¸ì—°ì˜ ë‹´ë°±í•¨ì´ ì‚´ì•„ìžˆëŠ” ê²ƒì´ íŠ¹ì§•ìž…ë‹ˆë‹¤.\n",
      "\n",
      "ì™€ì¸ ì¶”ì²œ:\n",
      "ì´ í’ì„±í•˜ê³  ê³ ì†Œí•œ ì‚¼ê²¹ì‚´ì—ëŠ” ì§€ë°©ì˜ ê¸°ë¦„ì§„ ë§›ì„ ì‚°ëœ»í•˜ê²Œ ìž¡ì•„ì¤„ ìˆ˜ ìžˆëŠ” ì™€ì¸ì´ ì•„ì£¼ ì¢‹ìŠµë‹ˆë‹¤.\n",
      "\n",
      "1.  **ë¯¸ë””ì—„ ë°”ë”” ë ˆë“œ ì™€ì¸ (ì˜ˆ: í‚¤ì•ˆí‹°, ê°€ë©”ì´):**\n",
      "    *   **ì„¤ëª…:** ì‚°ë¯¸ê°€ ì¢‹ê³  ê³¼ì¼ í–¥ì´ í’ë¶€í•œ ë ˆë“œ ì™€ì¸ì€ ì‚¼ê²¹ì‚´ì˜ ê¸°ë¦„ì§„ ë§›ì„ ê¹”ë”í•˜ê²Œ ì¤‘í™”ì‹œì¼œì£¼ë©´ì„œ, ê³ ê¸°ì˜ ê°ì¹ ë§›ì„ ë”ìš± ë‹ë³´ì´ê²Œ í•©ë‹ˆë‹¤. íŠ¹ížˆ ì´íƒˆë¦¬ì•„ì˜ í‚¤ì•ˆí‹° (ì‚°ì§€ì˜¤ë² ì œ)ë‚˜ í”„ëž‘ìŠ¤ì˜ ë³´ì¡¸ë ˆ(ê°€ë©”ì´)ëŠ” ì§€ë‚˜ì¹˜ê²Œ ë¬´ê²ì§€ ì•Šìœ¼ë©´ì„œë„ ë¯¸íŠ¸ íŒŒíŠ¸ë„ˆë¡œì„œ í›Œë¥­í•œ ê· í˜•ê°ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "2.  **ë“œë¼ì´ ìŠ¤íŒŒí´ë§ ì™€ì¸ (ì˜ˆ: ë¸Œë¤¼ ìƒ´íŽ˜ì¸, ê¹Œë°”):**\n",
      "    *   **ì„¤ëª…:** íƒ„ì‚°ì˜ ì²­ëŸ‰ê°ê³¼ ë†’ì€ ì‚°ë¯¸ê°€ ì‚¼ê²¹ì‚´ì˜ í’ë¶€í•œ ì§€ë°©ì„ íš¨ê³¼ì ìœ¼ë¡œ ì”»ì–´ë‚´ì–´ ìž…ì•ˆì„ ê°œìš´í•˜ê²Œ í•´ì¤ë‹ˆë‹¤. ë²„ë¸”ì´ ê¸°ë¦„ê¸°ë¥¼ ì •ëˆí•´ ì£¼ì–´ ë‹¤ìŒ í•œ ì ì„ ë” ë§›ìžˆê²Œ ì¦ê¸¸ ìˆ˜ ìžˆë„ë¡ ë„ì™€ì£¼ì£ . ì¶•ì œ ë¶„ìœ„ê¸°ë¥¼ ë”í•˜ê³  ì‹¶ì„ ë•Œë„ ì¢‹ì€ ì„ íƒìž…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "res = r1.invoke(input_data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7019493",
   "metadata": {},
   "source": [
    "# ìš”ë¦¬ë©”ë‰´ì— ì ì ˆí•œ Wine 5ê°œ ì¶”ì²œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "873d197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5d5c77",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      3\u001b[39m vector_db = PineconeVectorStore(embedding=embedding,\n\u001b[32m      4\u001b[39m                                 index_name=PINECONE_INDEX_NAME,\n\u001b[32m      5\u001b[39m                                   namespace=PINECONE_NAMESPACE)\n\u001b[32m      7\u001b[39m query = res\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mvector_db\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\langchain_pinecone\\vectorstores.py:676\u001b[39m, in \u001b[36mPineconeVectorStore.similarity_search\u001b[39m\u001b[34m(self, query, k, filter, namespace, **kwargs)\u001b[39m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimilarity_search\u001b[39m(\n\u001b[32m    658\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    659\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    663\u001b[39m     **kwargs: Any,\n\u001b[32m    664\u001b[39m ) -> List[Document]:\n\u001b[32m    665\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return pinecone documents most similar to query.\u001b[39;00m\n\u001b[32m    666\u001b[39m \n\u001b[32m    667\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    674\u001b[39m \u001b[33;03m        List of Documents most similar to the query and score for each\u001b[39;00m\n\u001b[32m    675\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m676\u001b[39m     docs_and_scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    679\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\langchain_pinecone\\vectorstores.py:543\u001b[39m, in \u001b[36mPineconeVectorStore.similarity_search_with_score\u001b[39m\u001b[34m(self, query, k, filter, namespace, **kwargs)\u001b[39m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimilarity_search_with_score\u001b[39m(\n\u001b[32m    524\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    525\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    529\u001b[39m     **kwargs: Any,\n\u001b[32m    530\u001b[39m ) -> List[Tuple[Document, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[32m    531\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return pinecone documents most similar to query, along with scores.\u001b[39;00m\n\u001b[32m    532\u001b[39m \n\u001b[32m    533\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    540\u001b[39m \u001b[33;03m        List of Documents most similar to the query and score for each\u001b[39;00m\n\u001b[32m    541\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    542\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.similarity_search_by_vector_with_score(\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    544\u001b[39m         k=k,\n\u001b[32m    545\u001b[39m         \u001b[38;5;28mfilter\u001b[39m=\u001b[38;5;28mfilter\u001b[39m,\n\u001b[32m    546\u001b[39m         namespace=namespace,\n\u001b[32m    547\u001b[39m         **kwargs,\n\u001b[32m    548\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:759\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_query\u001b[39m\u001b[34m(self, text, **kwargs)\u001b[39m\n\u001b[32m    749\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Call out to OpenAI's embedding endpoint for embedding query text.\u001b[39;00m\n\u001b[32m    750\u001b[39m \n\u001b[32m    751\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    756\u001b[39m \u001b[33;03m    Embedding for the text.\u001b[39;00m\n\u001b[32m    757\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    758\u001b[39m \u001b[38;5;28mself\u001b[39m._ensure_sync_client_available()\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:709\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    706\u001b[39m \u001b[38;5;66;03m# Unconditionally call _get_len_safe_embeddings to handle length safety.\u001b[39;00m\n\u001b[32m    707\u001b[39m \u001b[38;5;66;03m# This could be optimized to avoid double work when all texts are short enough.\u001b[39;00m\n\u001b[32m    708\u001b[39m engine = cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m.deployment)\n\u001b[32m--> \u001b[39m\u001b[32m709\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:576\u001b[39m, in \u001b[36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[39m\u001b[34m(self, texts, engine, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    574\u001b[39m \u001b[38;5;66;03m# Make API call with this batch\u001b[39;00m\n\u001b[32m    575\u001b[39m batch_tokens = tokens[i:batch_end]\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    578\u001b[39m     response = response.model_dump()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\openai\\resources\\embeddings.py:132\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    127\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m             ).tolist()\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "embedding = OpenAIEmbeddings (model = OPENAI_EMBEDDING_MODEL)\n",
    "\n",
    "vector_db = PineconeVectorStore(embedding=embedding,\n",
    "                                index_name=PINECONE_INDEX_NAME,\n",
    "                                  namespace=PINECONE_NAMESPACE)\n",
    "\n",
    "query = res\n",
    "#top 5 ê²€ìƒ‰\n",
    "vector_db.similarity_search(query, k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f3d94bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(\"wine-reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5996875",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
