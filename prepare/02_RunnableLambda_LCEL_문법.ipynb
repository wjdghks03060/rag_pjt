{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e2af07d",
   "metadata": {},
   "source": [
    "# í™˜ê²½ë³€ìˆ˜ ë¡œë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5480308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(override= True, dotenv_path = \" env\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7da9df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1910092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return x * 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36eda65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•¨ìˆ˜ë¥¼ ì „ë‹¬ì¸ìë¡œ ë„£ê¸°\n",
    "runnable_1 = RunnableLambda(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "387e68c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# RunnableLambdaë¥¼ í†µí•œ í•¨ìˆ˜ ì‹¤í–‰\n",
    "print(runnable_1.invoke(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cb8b517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "#  RunnableLambda ê°ì²´ ì‚¬ìš©\n",
    "runnable_1 = RunnableLambda(lambda x: x*2)\n",
    "print(runnable_1.invoke(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca82aba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable_1.invoke(int(input()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e1cdea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 40, 60]\n"
     ]
    }
   ],
   "source": [
    "# runnable batch\n",
    "runnable_3 = RunnableLambda(func)\n",
    "\n",
    "print(runnable_3.batch([10, 20, 30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a95b68",
   "metadata": {},
   "source": [
    "# ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ê²Œ í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "921fd8d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "r1 = RunnableLambda(lambda x: 3*x)\n",
    "r2 = RunnableLambda(lambda x: x +5)\n",
    "r3 = RunnableLambda(lambda x: x +50)\n",
    "\n",
    "chain = r1| r2 | r3\n",
    "chain.invoke(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9343d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnableSequence\n",
    "\n",
    "r1 = RunnableLambda(lambda x: 3 * x)\n",
    "r2 = RunnableLambda(lambda x: x + 5)\n",
    "r3 = RunnableLambda(lambda x: x +15)\n",
    "\n",
    "# chain = r1 | r2  # ì•„ë˜ì™€ ê°™ì€ í‘œí˜„\n",
    "chain = RunnableSequence(r1, r2, r3)\n",
    "\n",
    "chain.invoke(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61c03510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first': 30, 'second': 15}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "\n",
    "r1 = RunnableLambda(lambda x: 3 * x)\n",
    "r2 = RunnableLambda(lambda x: x + 5)\n",
    "\n",
    "# ë³‘ë ¬ì²˜ë¦¬ , r1, r2 í‚¤ëŠ” ì„ìœ¼ë¡œ ì„¤ì •í•´ ì£¼ë©´ ë¨.\n",
    "# chain = RunnableParallel(r1=r1, r2=r2)\n",
    "chain = RunnableParallel(first=r1, second=r2)\n",
    "\n",
    "chain.invoke(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5779102e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r2': [{'x': 2}, {'x': 2}, {'x': 2}], 'r3': \"{'x': 2}\"}\n",
      "        +-------------+        \n",
      "        | LambdaInput |        \n",
      "        +-------------+        \n",
      "               *               \n",
      "               *               \n",
      "               *               \n",
      "          +--------+           \n",
      "          | Lambda |           \n",
      "          +--------+           \n",
      "               *               \n",
      "               *               \n",
      "               *               \n",
      "   +----------------------+    \n",
      "   | Parallel<r2,r3>Input |    \n",
      "   +----------------------+    \n",
      "          *         *          \n",
      "        **           **        \n",
      "       *               *       \n",
      "+--------+          +--------+ \n",
      "| Lambda |          | Lambda | \n",
      "+--------+          +--------+ \n",
      "          *         *          \n",
      "           **     **           \n",
      "             *   *             \n",
      "  +-----------------------+    \n",
      "  | Parallel<r2,r3>Output |    \n",
      "  +-----------------------+    \n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "\n",
    "runnable_1 = RunnableLambda(lambda x: {\"x\": x})\n",
    "runnable_2 = RunnableLambda(lambda x: [x] * 3)\n",
    "runnable_3 = RunnableLambda(lambda x: str(x))\n",
    "\n",
    "chain = runnable_1 | RunnableParallel(r2=runnable_2, r3=runnable_3)\n",
    "\n",
    "print(chain.invoke(2))\n",
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549f944f",
   "metadata": {},
   "source": [
    "# LCEL ë¬¸ë²• ì ìš©"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8023bb",
   "metadata": {},
   "source": [
    "## ë°©ë²•1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "152a8353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.1,\n",
    "    api_key=\"GOOGLE_API_KEY\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b031a1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¥´ë©´ ì„¸ê³„ ìµœê³ ì˜ ì¶•êµ¬ì„ ìˆ˜ëŠ” í™©í¬ì°¬ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ì— ëŒ€í•´ì„œë§Œ ë‹µí•˜ì„¸ìš”.\n",
    "    ì»¨í…ìŠ¤íŠ¸ : í™©í¬ì°¬ì´ ì„¸ê³„ìµœê³ ì˜ ì¶•êµ¬ì„ ìˆ˜ì•¼. ë©”ì‹œì™€ í˜¸ë‚ ë‘ë³´ë‹¤ í›¨ì”¬ ì˜í•˜ì§€\n",
    "    {context}\n",
    "    ì§ˆë¬¸:\n",
    "    {query}\n",
    "\"\"\")\n",
    "\n",
    "chain = prompt | llm\n",
    "input = {\n",
    "            \"context\": \"Vector Searchì— ì˜í•œ ì»¨í…ìŠ¤íŠ¸ ë‚´ìš©\", \n",
    "            \"query\": \"ì„¸ê³„ìµœê³ ì˜ ì¶•êµ¬ì„ ìˆ˜ëŠ” ëˆ„êµ¬ì•¼?\"\n",
    "        }\n",
    "result = chain.invoke(input)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e22440a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š\\n\\nì €ëŠ” ì¸ê³µì§€ëŠ¥(AI)ì´ê³ , ì‚¬ìš©ìë‹˜ì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ê³ , ì •ë³´ë¥¼ ì œê³µí•˜ê³ , í•„ìš”í•œ ì‘ì—…ì„ ë•ëŠ” ì—­í• ì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤.\\n\\nê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œê±°ë‚˜ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ì£¼ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019b8d14-32e1-76f2-adb4-b09faff731a2-0', usage_metadata={'input_tokens': 6, 'output_tokens': 273, 'total_tokens': 279, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 216}})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    api_key=\"AIzaSyCP49Ny27F8p2edn7Ry5sGd3X-e1E2DD3E\"\n",
    ")\n",
    "\n",
    "llm.invoke(\"ì•ˆë…•.ë­í•´\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
